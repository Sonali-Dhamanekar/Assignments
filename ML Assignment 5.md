1. What are the key tasks that machine learning entails? What does data pre-processing imply?

3 core machine learning tasks: Classification, Regression, and Clustering. 

Data preprocessing/preparation/cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a dataset, or and refers to identifying incorrect, incomplete, irrelevant parts of the data and then modifying, replacing, or deleting the dirty or coarse data

2. Describe quantitative and qualitative data in depth. Make a distinction between the two.

Quantitative = Quantity

Quantitative data are

measures of values or counts and are expressed as numbers.
data about numeric variables (e.g. how many, how much or how often). 
Qualitative = Quality

Qualitative data are

measures of 'types' and may be represented by a name, symbol, or a number code.
Qualitative data are data about categorical variables (e.g. what type).
Data collected about a numeric variable will always be quantitative and data collected about a categorical variable will always be qualitative. Therefore, you can identify the type of data, prior to collection, based on whether the variable is numeric or categorical.

Quantitative and qualitative data provide different outcomes, and are often used together to get a full picture of a population. For example, if data are collected on annual income (quantitative), occupation data (qualitative) could also be gathered to get more detail on the average annual income for each type of occupation.  

Quantitative and qualitative data can be gathered from the same data unit depending on whether the variable of interest is numerical or categorical.

differences between qualitative vs. quantitative data

Quantitative data is numbers-based, countable, or measurable. Qualitative data is interpretation-based, descriptive, and relating to language.
Quantitative data tells us how many, how much, or how often in calculations. Qualitative data can help us to understand why, how, or what happened behind certain behaviors.
Quantitative data is fixed and universal. Qualitative data is subjective and unique.
Quantitative research methods are measuring and counting. Qualitative research methods are interviewing and observing.
Quantitative data is analyzed using statistical analysis. Qualitative data is analyzed by grouping the data into categories and themes.


3. Create a basic data collection that includes some sample records. Have at least one attribute from each of the machine learning data types.

Sources of 6 methods of data collection
There are many methods of data collection that you can use in your workplace, including:
1. Observation
Observational methods focus on examining things and collecting data about them. This might include observing individual animals or people in their natural spaces and places. Avoiding direct interactions between researchers and the subjects they are observing can ensure that results are more accurate.

Example: A children's store named Bubbly Baby is interested in developing a new children's toy to sell exclusively in their store. They want to make sure that they understand the toys babies like before developing product samples. The production team at Bubbly Baby plans to conduct observational research with babies, whose parents have provided consent, to examine what toys interest them the most. During the observation sessions, the production team stands in another room to view the toy selections each baby makes. This recorded information may then guide their development process and help them develop a new toy with components of each one the babies were interested in.

2. Survey
Survey methods focus on gathering written or multiple choice answers about various subjects from individuals. Typically, individuals interact with these questions online and there is little to no interaction between survey distributors and survey respondents. Companies may use them to gather quick internal or external feedback.
Example: A small car rental company called Rachel's Car Rentals is interested in learning more about customers' perceptions and loyalty for renting a car with them. To expand their reach and maintain cost efficiency, they choose to create and send out a survey to all of their customers from the past six months with a deadline of two weeks. The survey includes multiple choice and short answer questions, as well as spaces for customers to provide additional comments if they have more insight to share. Once the two weeks have passed and the company has collected all customer data, they can analyze it and decide how to use their findings.

3. Focus group
Focus group methods focus on gathering information directly from users. This method usually focuses more on feelings, opinions or emotions rather than statistics. Companies may use focus groups to better understand their consumers.
Example: Green Wicker University is considering a brand refresh for their university brand but wants to ensure that their target audiences will enjoy the brand's new image. The university designers put together some mockup brand logos and materials to share with focus group participants to gauge their perceptions. During this session, someone from the marketing team may run the session as a moderator and stay with participants to present them with each potential refresh idea. Before moving on to the next one, they may stop and ask participants what they think or feel about what they are seeing. Green Wicker University can then use those results to help guide the image of their brand refresh.

4. Interview
Interview methods can be more personal and involve face-to-face discussions about a topic between the researcher and participant. Researchers might share the questions with participants before interview sessions to allow them to decide if they feel comfortable taking part. This method may include gathering consent forms for video or audio recordings.
Example: Knit-a-little-bit, an instructional series focused on teaching people how to knit at different levels, worries that they aren't gaining customers at their projected rate. To understand the reasoning behind this, the company arranges for interviews with potential customers to listen to their perspectives. During each interview, the researcher asks participants questions and records their answers. Once interviewers record and analyze the data collected from all interviews, the company may use it to help boost their position in the market or make updates to their brand strategy.

5. Design thinking
Design thinking methods may focus on brainstorming with participants to generate unique ideas or solutions. Companies might use this if they are interested in solving challenges consumers face on their journey as product users. These sessions can happen face-to-face or virtually depending on where researchers and participants are located.
Example: Meditative Monkey, a meditation company, is interested in developing a new product specifically for individuals who struggle to fall asleep at night. They tried to brainstorm as a company, but want to gather more innovative ideas and decide to run a design thinking session with participants. First, they write a protocol as a guide to ensure that the session stays focused to gather as much information from participants as possible. Their protocol contains a script and guided steps for the thinking process, including: Write all the ideas you have, group them and finally vote for the ones you like best. After the session, Meditative Monkey can review the ideas they received and potentially use them for prototyping their new product.

6. User testing
Companies usually use user testing during or after the development of products or services. If they choose to use it during development, it might be to determine where users find the product challenging to navigate. They might also use it after they have already released a product or service if they are interested in making updates.
Example: Baller Bingo, a bingo game for smartphones, is interested in making updates to their application. First, they want to understand where users specifically want improvements and choose to use user testing methods. During their sessions, they ask participants to engage with all aspects of the application and then ask them what navigation or features they might like to see improved. Baller Bingo can then take this information to implement updates to their game.

4. What are the various causes of machine learning data issues? What are the ramifications?

Noisy data, incomplete data, inaccurate data, and unclean data lead to less accuracy in classification and low-quality results. Hence, data quality can also be considered as a major common problem while processing machine learning algorithms.
Ramification: a related or derived subject, problem, etc.; outgrowth; consequence; implication: The new tax law proved to have many ramifications unforeseen by the lawmakers. Botany. a structure formed of branches. a configuration of branching parts.


5. Demonstrate various approaches to categorical data exploration with appropriate examples.

Categorical Variable/Data (or Nominal variable): Such variables take on a fixed and limited number of possible values. For example – grades, gender, blood group type, etc. Also, in the case of categorical variables, the logical order is not the same as categorical data e.g. “one”, “two”, “three”. But the sorting of these variables uses logical order. For example, gender is a categorical variable and has categories – male and female and there is no intrinsic ordering to the categories. A purely categorical variable is one that simply allows you to assign categories, but you cannot clearly order the variables.

The most common graphical tool used to summarise a categorical variable is a bar chart. A bar chart (or bar graph) is a plot that presents summaries of grouped data with rectangular bars. The lengths of the bars is proportional to the values they represent. When summarising a single categorical variable, the length of the bars should show the raw counts or proportions of each category.

Pie Charts : Frequency of each category plotted as pie or wedges. It is a circular graph, where the arc length of each slice is proportional to the quantity it represents. 

6. How would the learning activity be affected if certain variables have missing values? Having said that, what can be done about it?

It can reduce the predictive accuracy of the model. Missing values may prevent the model from generalizing correctly and lead to biased results. It can introduce bias into the patterns in the dataset. This can reduce the model's ability to fit real-world data and reduce the reliability of predictions.

Missing values can pose a significant challenge in data analysis, as they can:

Reduce the sample size: This can decrease the accuracy and reliability of your analysis.

Introduce bias: If the missing data is not handled properly, it can bias the results of your analysis.

Make it difficult to perform certain analyses: Some statistical techniques require complete data for all variables, making them inapplicable when missing values are present

Removing Rows with Missing Values

Simple and efficient: Removes data points with missing values altogether.

Reduces sample size: Can lead to biased results if missingness is not random.

Not recommended for large datasets: Can discard valuable information.

Imputation Methods

Replacing missing values with estimated values.

Preserves sample size: Doesn’t reduce data points.

Can introduce bias: Estimated values might not be accurate.

Here are some common imputation methods:

1- Mean, Median, and Mode Imputation:

Replace missing values with the mean, median, or mode of the relevant variable.

Simple and efficient: Easy to implement.

Can be inaccurate: Doesn’t consider the relationships between variables.

2. Forward and Backward Fill

Replace missing values with the previous or next non-missing value in the same variable.

Simple and intuitive: Preserves temporal order.

Can be inaccurate: Assumes missing values are close to observed values

3. Interpolation Techniques

Estimate missing values based on surrounding data points using techniques like linear interpolation or spline interpolation.

More sophisticated than mean/median imputation: Captures relationships between variables.

Requires additional libraries and computational resources.



7. Describe the various methods for dealing with missing data values in depth.

Removing Rows with Missing Values

Simple and efficient: Removes data points with missing values altogether.

Reduces sample size: Can lead to biased results if missingness is not random.

Not recommended for large datasets: Can discard valuable information.

Imputation Methods

Replacing missing values with estimated values.

Preserves sample size: Doesn’t reduce data points.

Can introduce bias: Estimated values might not be accurate.

Here are some common imputation methods:

1- Mean, Median, and Mode Imputation:

Replace missing values with the mean, median, or mode of the relevant variable.

Simple and efficient: Easy to implement.

Can be inaccurate: Doesn’t consider the relationships between variables.

2. Forward and Backward Fill

Replace missing values with the previous or next non-missing value in the same variable.

Simple and intuitive: Preserves temporal order.

Can be inaccurate: Assumes missing values are close to observed values

3. Interpolation Techniques

Estimate missing values based on surrounding data points using techniques like linear interpolation or spline interpolation.

More sophisticated than mean/median imputation: Captures relationships between variables.

Requires additional libraries and computational resources.



8. What are the various data pre-processing techniques? Explain dimensionality reduction and function selection in a few words.

Data preprocessing techniques can be grouped into three main categories: data cleaning, data transformation, and structural operations. These steps can happen in any order and iteratively.

Dimensionality reduction refers to the method of reducing variables in a training dataset used to develop machine learning models. The process keeps a check on the dimensionality of data by projecting high dimensional data to a lower dimensional space that encapsulates the 'core essence' of the data.

9.

i. What is the IQR? What criteria are used to assess it?

ii. Describe the various components of a box plot in detail? When will the lower whisker
surpass the upper whisker in length? How can box plots be used to identify outliers?

i. What is the IQR? What criteria are used to assess it?

Interquartile Range Definition
The interquartile range defines the difference between the third and the first quartile. Quartiles are the partitioned values that divide the whole series into 4 equal parts. So, there are 3 quartiles.

Step 1: Sort your data from low to high

Step 2: Identify the median, the first quartile (Q1), and the third quartile (Q3)

Step 3: Calculate your IQR

Step 4: Calculate your upper fence

Step 5: Calculate your lower fence

Step 6: Use your fences to highlight any outliers


ii. Describe the various components of a box plot in detail? When will the lower whisker surpass the upper whisker in length? How can box plots be used to identify outliers?

A box and whisker plot—also called a box plot—displays the five-number summary of a set of data. The five-number summary is the minimum, first quartile, median, third quartile, and maximum. In a box plot, we draw a box from the first quartile to the third quartile. A vertical line goes through the box at the median.

when the boxplots of several samples are lined up alongside one another (Parallel Boxplots). The box length gives an indication of the sample variability and the line across the box shows where the sample is centred. The position of the box in its whiskers and the position of the line in the box also tells us whether the sample is symmetric or skewed, either to the right or left. For a symmetric distribution, long whiskers, relative to the box length, can betray a heavy tailed population and short whiskers, a short tailed population. So, provided the number of points in the sample is not too small, the boxplot also gives us some idea of the "shape" of the sample, and by implication, the shape of the population from which it was drawn. This is all important when considering appropriate analyses of the data.

Box plots help you identify interesting data points, or outliers. These values are plotted as data points and fall beyond the whiskers. Figure 8 shows a box plot that has three outliers, shown as red dots above the upper whisker. These three points are more than 1.5 times the IQR.

10. Make brief notes on any two of the following:

  1. Data collected at regular intervals

  2. The gap between the quartiles

  3. Use a cross-tab


1. Data collected at regular intervals

Interval data, also called an integer, is defined as a data type which is measured along a scale, in which each point is placed at equal distance from one another. Interval data always appears in the form of numbers or numerical values where the distance between the two points is standardized and equal.

Interval data cannot be multiplied or divided, however, it can be added or subtracted. Interval data is measured on an interval scale. A simple example of interval data: The difference between 100 degrees Fahrenheit and 90 degrees Fahrenheit is the same as 60 degrees Fahrenheit and 70 degrees Fahrenheit.

In market research or in any other forms of social, economic or business research interval data plays a pivotal role. What makes interval data so popular and in-demand is because interval data supports almost all statistical test and transformations in obtaining quantitative data.

Interval data has very distinctive attributes that make it distinct in comparison to nominal data, ordinal data or even ratio data. Interval data doesn’t have a defined absolute zero point which is present in ratio data. The lack of absolute point zero makes comparisons of direct magnitudes impossible. For example, Object A is twice as large as Object B is not a possibility in interval data.

2. The gap between the quartiles

The distance between the first and third quartiles—the interquartile range (IQR)—is a measure of variability. It indicates the spread of the middle 50% of the data. The IQR is an especially good measure of variability for skewed distributions or distributions with outliers.

Quartiles are a type of percentile. A percentile is a value with a certain percentage of the data falling below it. In general terms, k% of the data falls below the kth percentile.

The first quartile (Q1, or the lowest quartile) is the 25th percentile, meaning that 25% of the data falls below the first quartile.
The second quartile (Q2, or the median) is the 50th percentile, meaning that 50% of the data falls below the second quartile.
The third quartile (Q3, or the upper quartile) is the 75th percentile, meaning that 75% of the data falls below the third quartile.
By splitting the data at the 25th, 50th, and 75th percentiles, the quartiles divide the data into four equal parts.

In a sample or dataset, the quartiles divide the data into four groups with equal numbers of observations.
In a probability distribution, the quartiles divide the distribution’s range into four intervals with equal probability.


